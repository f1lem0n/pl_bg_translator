{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tłumacz polsko-bułgarski oparty o architekturę transformera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biblioteki i ustawienia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import get_dataset_splits, random_eval, get_batch_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# model hyperparameters\n",
    "EMBEDDING_SIZE = 128\n",
    "NUM_ENC_LAYERS = 3\n",
    "NUM_DEC_LAYERS = 3\n",
    "TOKEN_LIMIT = 32\n",
    "FWD_EXPANSION = 4\n",
    "NUM_HEADS = 16\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# other\n",
    "ENABLE_TRAINING = True\n",
    "TRAIN_DATASET_PATH = f\"./datasets/train_dataset_sub_{TOKEN_LIMIT}.pkl\"\n",
    "TEST_DATASET_PATH = f\"./datasets/test_dataset_sub_{TOKEN_LIMIT}.pkl\"\n",
    "MODEL_PATH = f\"./serialized_models/model_sub_{TOKEN_LIMIT}_{NUM_EPOCHS}\"\n",
    "CORPUS_PATH = \"./raw_data/subset/\"\n",
    "TFMT = \"%M:%S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata = {\n",
    "    \"training_hyperparameters\": {\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "    },\n",
    "    \"model_hyperparameters\": {\n",
    "        \"embedding_size\": EMBEDDING_SIZE,\n",
    "        \"num_enc_layers\": NUM_ENC_LAYERS,\n",
    "        \"num_dec_layers\": NUM_DEC_LAYERS,\n",
    "        \"token_limit\": TOKEN_LIMIT,\n",
    "        \"fwd_expansion\": FWD_EXPANSION,\n",
    "        \"num_heads\": NUM_HEADS,\n",
    "        \"dropout\": DROPOUT,\n",
    "        \"device\": str(DEVICE),\n",
    "    },\n",
    "    \"model_architecture\": {},\n",
    "    \"other\": {\n",
    "        \"model_path\": MODEL_PATH,\n",
    "        \"train_dataset_path\": TRAIN_DATASET_PATH,\n",
    "        \"test_dataset_path\": TEST_DATASET_PATH,\n",
    "        \"corpus_path\": CORPUS_PATH,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaprojektowałem dwa tokenizatory, osobno dla każdego języka. Są one oparte o modele `pl_core_news_lg` i `uk_core_news_lg` z biblioteki `spacy`. Jak widać dla języka bułgarskiego użyty został model przeznaczony dla języka ukraińskiego. Jest to spowodowane brakiem modelu dla języka bułgarskiego w bibliotece `spacy` oraz wysokim podobieństwem tych języków. Zakładam, że nie powinno to negatywnie wpłynąć na jakość tłumaczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_pl = spacy.load(\"pl_core_news_lg\")\n",
    "nlp_bg = spacy.load(\"uk_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pl(text):\n",
    "    return [token.text for token in nlp_pl.tokenizer(text)]\n",
    "\n",
    "def tokenize_bg(text):\n",
    "    return [token.text for token in nlp_bg.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polish = Field(\n",
    "    tokenize=tokenize_pl, lower=True,\n",
    "    init_token=\"<bos>\", eos_token=\"<eos>\"\n",
    ")\n",
    "\n",
    "bulgarian = Field(\n",
    "    tokenize=tokenize_bg, lower=True,\n",
    "    init_token=\"<bos>\", eos_token=\"<eos>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(\"src\", polish), (\"tgt\", bulgarian)]\n",
    "train_dataset, test_dataset = get_dataset_splits(\n",
    "    train_dataset_path=TRAIN_DATASET_PATH,\n",
    "    test_dataset_path=TEST_DATASET_PATH,\n",
    "    corpus_path=CORPUS_PATH,\n",
    "    max_len=TOKEN_LIMIT,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polish.build_vocab(train_dataset, max_size=10000, min_freq=2)\n",
    "bulgarian.build_vocab(train_dataset, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_dataset, test_dataset),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    sort_key = lambda x: len(x.src),\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architektura modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PL2BG(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        token_limit,\n",
    "        device,\n",
    "    ):\n",
    "        super(PL2BG, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.tgt_word_embedding = nn.Embedding(tgt_vocab_size, embedding_size)\n",
    "        self.position_embedding = nn.Embedding(token_limit, embedding_size)\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=forward_expansion,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.fc_1 = nn.Linear(embedding_size, 512)\n",
    "        self.fc_2 = nn.Linear(512, 512)\n",
    "        self.fc_3 = nn.Linear(512, 1024)\n",
    "        self.fc_4 = nn.Linear(1024, embedding_size)\n",
    "        self.unembedding = nn.Linear(embedding_size, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        return src.transpose(0, 1) == self.src_pad_idx\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_seq_len, n = src.shape\n",
    "        tgt_seq_len = tgt.shape[0]\n",
    "        src_positions = torch.arange(0, src_seq_len) \\\n",
    "            .unsqueeze(1).expand(src_seq_len, n).to(self.device)\n",
    "        tgt_positions = torch.arange(0, tgt_seq_len) \\\n",
    "            .unsqueeze(1).expand(tgt_seq_len, n).to(self.device)\n",
    "        embedded_src = self.dropout(\n",
    "            self.src_word_embedding(src)\n",
    "            + self.position_embedding(src_positions)\n",
    "        )\n",
    "        embedded_tgt = self.dropout(\n",
    "            self.tgt_word_embedding(tgt)\n",
    "            + self.position_embedding(tgt_positions)\n",
    "        )\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.transformer \\\n",
    "            .generate_square_subsequent_mask(tgt_seq_len).to(self.device)\n",
    "        output = self.transformer(\n",
    "            embedded_src,\n",
    "            embedded_tgt,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=tgt_mask\n",
    "        )\n",
    "        output = F.relu(self.fc_1(output))\n",
    "        output = F.relu(self.fc_2(output))\n",
    "        output = F.relu(self.fc_3(output))\n",
    "        output = F.relu(self.fc_4(output))\n",
    "        return self.unembedding(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicjalizacja i trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_pad_idx = bulgarian.vocab.stoi[\"<pad>\"]\n",
    "src_pad_idx = polish.vocab.stoi[\"<pad>\"]\n",
    "tgt_vocab_size = len(bulgarian.vocab)\n",
    "src_vocab_size = len(polish.vocab)\n",
    "\n",
    "model = PL2BG(\n",
    "    num_encoder_layers=NUM_ENC_LAYERS,\n",
    "    num_decoder_layers=NUM_DEC_LAYERS,\n",
    "    forward_expansion=FWD_EXPANSION,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    src_pad_idx=src_pad_idx,\n",
    "    token_limit=TOKEN_LIMIT,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT,\n",
    "    device=DEVICE,\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [layer.view(1, -1).squeeze().shape[0] for layer in model.parameters()]\n",
    "model_metadata[\"model_architecture\"][\"num_params\"] = sum(layers)\n",
    "model_metadata[\"model_architecture\"][\"num_layers\"] = len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_TRAINING:\n",
    "    print(\"Training...\")\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": [],\n",
    "        \"train_bleu\": [],\n",
    "        \"test_bleu\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        model.train()\n",
    "        mean_train_bleu = 0\n",
    "        mean_test_bleu = 0\n",
    "        mean_loss = 0\n",
    "\n",
    "        epoch_num = f\"[{epoch + 1:0{len(str(NUM_EPOCHS))}d} / {NUM_EPOCHS}]\"\n",
    "        iterator = tqdm(train_iterator, desc=f\"EPOCH {epoch_num}\")\n",
    "        for idx, batch in enumerate(iterator):\n",
    "            src = batch.src.to(DEVICE)\n",
    "            tgt = batch.tgt.to(DEVICE)\n",
    "\n",
    "            out = model(src, tgt[:-1])\n",
    "            out = out.reshape(-1, out.shape[2])\n",
    "            _tgt = tgt[1:].reshape(-1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = criterion(out, _tgt)\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            current_train_bleu = get_batch_bleu(model, src, tgt, bulgarian)\n",
    "            current_test_bleu = random_eval(\n",
    "                model=model,\n",
    "                n_examples=32,\n",
    "                dataset=test_dataset,\n",
    "                src_lang=polish,\n",
    "                tgt_lang=bulgarian,\n",
    "                device=DEVICE,\n",
    "                token_limit=TOKEN_LIMIT,\n",
    "            )[\"score\"]\n",
    "\n",
    "            mean_loss += current_loss / len(train_iterator)\n",
    "            mean_train_bleu += current_train_bleu / len(train_iterator)\n",
    "            mean_test_bleu += current_test_bleu / len(train_iterator)\n",
    "\n",
    "            if idx == len(train_iterator):\n",
    "                iterator.set_postfix_str(\n",
    "                    f\"loss={mean_loss:.4f}\\t\"\n",
    "                    f\"train_bleu={mean_train_bleu:.2f}\\t\"\n",
    "                    f\"test_bleu={mean_test_bleu:.2f}\"\n",
    "                )\n",
    "            else:\n",
    "                iterator.set_postfix_str(\n",
    "                    f\"loss={current_loss:.6f}\\t\"\n",
    "                    f\"train_bleu={current_train_bleu:.4f}\\t\"\n",
    "                    f\"test_bleu={current_test_bleu:.4f}\"\n",
    "                )\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "            optimizer.step()\n",
    "\n",
    "        metrics[\"loss\"].append(mean_loss)\n",
    "        metrics[\"train_bleu\"].append(mean_train_bleu)\n",
    "        metrics[\"test_bleu\"].append(mean_test_bleu)\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    json.dump(model_metadata, open(MODEL_PATH + \".json\", \"w\"))\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "else:\n",
    "    print(\"Loading pretrained model...\")\n",
    "    print(model.load_state_dict(torch.load(MODEL_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(model_metadata, open(MODEL_PATH + \".json\", \"w\"))\n",
    "torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ENABLE_TRAINING:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax[0].plot(metrics[\"train_bleu\"], c=\"orange\")\n",
    "    ax[0].plot(metrics[\"test_bleu\"], c=\"green\")\n",
    "    ax[0].legend([\"Train BLEU\", \"Test BLEU\"])\n",
    "    ax[0].set_ylabel(\"Mean BLEU score\")\n",
    "    ax[0].set_xlabel(\"Epoch\")\n",
    "    ax[0].set_title(\"BLEU score over epochs\")\n",
    "    ax[1].plot(metrics[\"loss\"], c=\"orange\")\n",
    "    ax[1].set_ylabel(\"Mean loss\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].set_title(\"Loss over epochs\")\n",
    "    plt.savefig(f\"{MODEL_PATH}.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = random_eval(\n",
    "    model=model,\n",
    "    n_examples=32,\n",
    "    dataset=test_dataset,\n",
    "    src_lang=polish,\n",
    "    tgt_lang=bulgarian,\n",
    "    device=DEVICE,\n",
    "    token_limit=TOKEN_LIMIT,\n",
    ")\n",
    "for example in evaluation[\"examples\"]:\n",
    "    print(example, file=open(f\"{MODEL_PATH}.txt\", \"a\"))\n",
    "print(f\"BLEU: {evaluation[\"score\"]}\", file=open(f\"{MODEL_PATH}.txt\", \"a\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
